{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shubham-MLwiz/Hello-World/blob/master/pretrain_1_5b_gpt_10ks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code is heavily inspired from [Sebastian Raschka](https://x.com/rasbt)'s excellent book, \"[Build a Large Language Model (From Scratch)](https://www.manning.com/books/build-a-large-language-model-from-scratch)\".\n",
        "\n",
        "I highly recommend buying and reading that book.\n",
        "\n",
        "My code here is taken directly from Sebastian's book, but with some slight variable and styling updates.\n",
        "\n",
        "Questions?  Feel free to DM me on [Twitter](https://x.com/virattt)."
      ],
      "metadata": {
        "id": "M5bRPAw1n_oG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install financial-datasets --quiet\n",
        "!pip install tiktoken --quiet"
      ],
      "metadata": {
        "id": "HuZEvoAjxYNZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91596f04-cd59-4fcd-e80b-5401a39b8069"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.3/107.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.3/409.3 kB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m83.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.9/362.9 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m79.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.9/302.9 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.0/149.0 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Setup the config"
      ],
      "metadata": {
        "id": "Xg68QogFtspy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yvXrPrU4q--F"
      },
      "outputs": [],
      "source": [
        "GPT_CONFIG_1558M = {\n",
        "    \"vocab_size\": 50257,\n",
        "    \"context_length\": 1024,\n",
        "    \"emb_dim\": 1600,\n",
        "    \"n_heads\": 25,\n",
        "    \"n_layers\": 48,\n",
        "    \"drop_rate\": 0.1,\n",
        "    \"qkv_bias\": False\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Implement LayerNorm"
      ],
      "metadata": {
        "id": "L8ILrmmrJjmv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self, emb_dim):\n",
        "    super().__init__()\n",
        "    self.eps = 1e-5\n",
        "    self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "    self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "  def forward(self, x):\n",
        "    mean = x.mean(dim=-1, keepdim=True)\n",
        "    var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "    normalized_x = (x - mean) / torch.sqrt(var + self.eps)\n",
        "    return self.scale * normalized_x + self.shift"
      ],
      "metadata": {
        "id": "xqai_MIMJepw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Implement feed forward network with GELU activations"
      ],
      "metadata": {
        "id": "7UMk2RGQPKeX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GELU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1 + torch.tanh(\n",
        "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
        "            (x + 0.044715 * torch.pow(x, 3))\n",
        "        ))"
      ],
      "metadata": {
        "id": "loslDi27PNdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement feed-forward neural network\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "        nn.Linear(config[\"emb_dim\"], 4 * config[\"emb_dim\"]),\n",
        "        GELU(),\n",
        "        nn.Linear(4 * config[\"emb_dim\"], config[\"emb_dim\"]),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layers(x)"
      ],
      "metadata": {
        "id": "5ROLTWSJW55K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Implement multi-head attention"
      ],
      "metadata": {
        "id": "TYxPkbofY0gB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "    super().__init__()\n",
        "\n",
        "    assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
        "\n",
        "    self.d_out = d_out                  # 768\n",
        "    self.num_heads = num_heads          # 12\n",
        "    self.head_dim = d_out // num_heads  # 64\n",
        "    self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self._out_proj = nn.Linear(d_out, d_out)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.register_buffer(\n",
        "        'mask',\n",
        "        torch.triu(torch.ones(\n",
        "            context_length,             # 1024\n",
        "            context_length,             # 1024\n",
        "          ), diagonal=1)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    batch_size, num_tokens, embedding_length = x.shape\n",
        "    keys = self.W_key(x)\n",
        "    queries = self.W_query(x)\n",
        "    values = self.W_value(x)\n",
        "\n",
        "    # Add the num_heads and head_dim dimensions\n",
        "    keys = keys.view(batch_size, num_tokens, self.num_heads, self.head_dim)       # Transform to a tensor of dimensions: 2 x 1024 x 12 x 64\n",
        "    queries = queries.view(batch_size, num_tokens, self.num_heads, self.head_dim) # Transform to a tensor of dimensions: 2 x 1024 x 12 x 64\n",
        "    values = values.view(batch_size, num_tokens, self.num_heads, self.head_dim)   # Transform to a tensor of dimensions: 2 x 1024 x 12 x 64\n",
        "\n",
        "    # Transpose from (batch_size, num_tokens, num_heads, head_dim) to (batch_size, num_heads, num_tokens, head_dim)\n",
        "    queries = queries.transpose(1, 2)\n",
        "    keys = keys.transpose(1, 2)\n",
        "    values = values.transpose(1, 2)\n",
        "\n",
        "    # Calculate attention scores\n",
        "    attention_scores = queries @ keys.transpose(2, 3)\n",
        "    mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "\n",
        "    # Mask the attention scores\n",
        "    attention_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "    # Calculate attention weights\n",
        "    attention_weights = torch.softmax(attention_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "\n",
        "    # Apply dropout to attention weights\n",
        "    attention_weights = self.dropout(attention_weights)\n",
        "\n",
        "    # Calculate context vectors\n",
        "    context_vectors = (attention_weights @ values).transpose(1, 2)\n",
        "\n",
        "    # Concatenate the context vectors\n",
        "    context_vectors = context_vectors.contiguous().view(batch_size, num_tokens, self.d_out)\n",
        "    return self._out_proj(context_vectors)"
      ],
      "metadata": {
        "id": "N3eU4P13Y2ws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Implement the Transformer block"
      ],
      "metadata": {
        "id": "jBhdcB3hYXq2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "\n",
        "    self.attention = MultiHeadAttention(\n",
        "        d_in=config[\"emb_dim\"],\n",
        "        d_out=config[\"emb_dim\"],\n",
        "        context_length=config[\"context_length\"],\n",
        "        dropout=config[\"drop_rate\"],\n",
        "        num_heads=config[\"n_heads\"],\n",
        "        qkv_bias=config[\"qkv_bias\"]\n",
        "    )\n",
        "\n",
        "    self.ff = FeedForward(config)\n",
        "    self.norm1 = LayerNorm(config[\"emb_dim\"])\n",
        "    self.norm2 = LayerNorm(config[\"emb_dim\"])\n",
        "    self.drop_shortcut = nn.Dropout(config[\"drop_rate\"])\n",
        "\n",
        "  def forward(self, x):\n",
        "    shortcut = x\n",
        "\n",
        "    # Attention layer\n",
        "    x = self.norm1(x)\n",
        "    x = self.attention(x)\n",
        "    x = self.drop_shortcut(x)\n",
        "    x = x + shortcut         # Add the original input back\n",
        "\n",
        "    # Feedforward layer\n",
        "    shortcut = x\n",
        "    x = self.norm2(x)\n",
        "    x = self.ff(x)\n",
        "    x = self.drop_shortcut(x)\n",
        "    x = x + shortcut         # Add the original input back\n",
        "    return x"
      ],
      "metadata": {
        "id": "JnzH0rp4Yg76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Implement the GPT model"
      ],
      "metadata": {
        "id": "KGmpcEtyFWau"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTModel(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "\n",
        "    self.token_embedding = nn.Embedding(config[\"vocab_size\"], config[\"emb_dim\"])\n",
        "    self.positional_embedding = nn.Embedding(config[\"context_length\"], config[\"emb_dim\"])\n",
        "    self.drop_embedding = nn.Dropout(config[\"drop_rate\"])\n",
        "\n",
        "    self.transformer_blocks = nn.Sequential(\n",
        "        *[TransformerBlock(config) for _ in range(config[\"n_layers\"])]\n",
        "    )\n",
        "\n",
        "    self.final_norm = LayerNorm(config[\"emb_dim\"])\n",
        "    self.out_head = nn.Linear(config[\"emb_dim\"], config[\"vocab_size\"], bias=False)\n",
        "\n",
        "  def forward(self, in_idx):\n",
        "    batch_size, sequence_length = in_idx.shape\n",
        "    token_embeddings = self.token_embedding(in_idx)\n",
        "    positional_embeddings = self.positional_embedding(\n",
        "        torch.arange(sequence_length, device=in_idx.device)\n",
        "    )\n",
        "    x = token_embeddings + positional_embeddings\n",
        "    x = self.drop_embedding(x)\n",
        "\n",
        "    x = self.transformer_blocks(x)\n",
        "    x = self.final_norm(x)\n",
        "    logits = self.out_head(x)\n",
        "    return logits"
      ],
      "metadata": {
        "id": "WVfKzAAsgULA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT_CONFIG_1558M)"
      ],
      "metadata": {
        "id": "pOPKlmP3hLC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Generate some text"
      ],
      "metadata": {
        "id": "tFd1M8lCh2cQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "\n",
        "        logits = logits[:, -1, :]\n",
        "        probas = torch.softmax(logits, dim=-1)\n",
        "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)\n",
        "        idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "    return idx\n",
        "\n",
        "def text_to_token_ids(text, tokenizer):\n",
        "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
        "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
        "    # .unsqueeze(0) adds the batch dimension\n",
        "    return encoded_tensor\n",
        "\n",
        "def token_ids_to_text(token_ids, tokenizer):\n",
        "    flat = token_ids.squeeze(0) # Remove batch dimension\n",
        "    return tokenizer.decode(flat.tolist())\n"
      ],
      "metadata": {
        "id": "RcSZ7SbThZY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,\n",
        "    \"context_length\": 256,\n",
        "    \"emb_dim\": 768,\n",
        "    \"n_heads\": 12,\n",
        "    \"n_layers\": 12,\n",
        "    \"drop_rate\": 0.1,\n",
        "    \"qkv_bias\": False\n",
        "}\n",
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "4WsoMh8Whynf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "1816aee6-f9ce-4cfd-95b9-ad77546695ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPTModel(\n",
              "  (token_embedding): Embedding(50257, 768)\n",
              "  (positional_embedding): Embedding(256, 768)\n",
              "  (drop_embedding): Dropout(p=0.1, inplace=False)\n",
              "  (transformer_blocks): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (_out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (_out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (2): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (_out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (3): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (_out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (4): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (_out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (5): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (_out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (6): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (_out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (7): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (_out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (8): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (_out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (9): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (_out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (10): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (_out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (11): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (_out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (final_norm): LayerNorm()\n",
              "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "start_context = \"Every effort moves you\"\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "token_ids = generate_text_simple(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(start_context, tokenizer),\n",
        "    max_new_tokens=10,\n",
        "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
        ")\n",
        "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYPj7rLu1snP",
        "outputId": "dce68d7a-ab96-4246-efb8-82213421c775"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output text:\n",
            " Every effort moves you rentingetic wasnم refres RexMeCHicular stren\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11. Create data loader"
      ],
      "metadata": {
        "id": "rkTjPOZ8fNH4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        " # Create a data loader\n",
        "class GPTDatasetV1(Dataset):\n",
        "    def __init__(self, txt, tokenizer, max_length, stride):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        token_ids = tokenizer.encode(txt)\n",
        "\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            # The input chunk\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            # The target chunk is the input chunk, offset by 1 character\n",
        "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
        "\n",
        "            # Append chunk to list of chunks\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]"
      ],
      "metadata": {
        "id": "EaTc0S57-lgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "def create_dataloader_v1(txt, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        drop_last=drop_last,\n",
        "        num_workers=0\n",
        "    )\n",
        "\n",
        "    return dataloader"
      ],
      "metadata": {
        "id": "JUqEda4w-fY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 12. Download our training text"
      ],
      "metadata": {
        "id": "dlU3lxoB-myU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from financial_datasets.parser import FilingParser\n",
        "\n",
        "parser = FilingParser()\n",
        "\n",
        "# AAPL 10-K\n",
        "aapl_10K = parser.get_10K_items(\n",
        "    ticker='AAPL',\n",
        "    year=2023,\n",
        "    item_names=['Item 1', 'Item 3', 'Item 7']\n",
        ")\n",
        "\n",
        "# NVDA 10-K\n",
        "nvda_10K = parser.get_10K_items(\n",
        "    ticker='NVDA',\n",
        "    year=2023,\n",
        "    item_names=['Item 1', 'Item 3', 'Item 7']\n",
        ")\n",
        "\n",
        "# MSFT 10-K\n",
        "msft_10K = parser.get_10K_items(\n",
        "    ticker='MSFT',\n",
        "    year=2023,\n",
        "    item_names=['Item 1', 'Item 3', 'Item 7']\n",
        ")"
      ],
      "metadata": {
        "id": "PzXbq-WbSa4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aapl_text = \" \".join([item for item in aapl_10K])\n",
        "nvda_text = \" \".join([item for item in nvda_10K])\n",
        "msft_text = \" \".join([item for item in msft_10K])\n",
        "\n",
        "text_data = f\"{aapl_text} {nvda_text} {msft_text}\""
      ],
      "metadata": {
        "id": "OucFQNLdTYY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_characters = len(text_data)\n",
        "total_tokens = len(tokenizer.encode(text_data))\n",
        "print(f\"Total characters: {total_characters}\")\n",
        "print(f\"Total tokens: {total_tokens}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JpEd3SsFTldh",
        "outputId": "54517ba4-08ec-45f4-8b11-341dd99ddd07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total characters: 258464\n",
            "Total tokens: 49933\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 13. Create training / validation data"
      ],
      "metadata": {
        "id": "3zvFDTkAbY3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ratio = 0.90 # 90% of data will be training, 10% will be validation\n",
        "split_index = int(train_ratio * len(text_data))\n",
        "train_data = text_data[:split_index]\n",
        "val_data = text_data[split_index:]"
      ],
      "metadata": {
        "id": "mJrhBAZpbrDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "train_loader = create_dataloader_v1(\n",
        "    train_data,\n",
        "    batch_size=2,\n",
        "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
        "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
        "    drop_last=True,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "val_loader = create_dataloader_v1(\n",
        "    val_data,\n",
        "    batch_size=2,\n",
        "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
        "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
        "    drop_last=False,\n",
        "    shuffle=False,\n",
        "    num_workers=0,\n",
        ")"
      ],
      "metadata": {
        "id": "m6vDv3rbb0Uw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 14. Implement loss functions\n"
      ],
      "metadata": {
        "id": "CakZ8idGcj9O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_loss_batch(input_batch, target_batch, model, device):\n",
        "  \"\"\"\n",
        "  Calculates the loss for a single batch.\n",
        "  \"\"\"\n",
        "  input_batch = input_batch.to(device)\n",
        "  target_batch = target_batch.to(device)\n",
        "\n",
        "  # Run the model\n",
        "  logits = model(input_batch)\n",
        "\n",
        "  # Calculate the loss\n",
        "  loss = torch.nn.functional.cross_entropy(\n",
        "      logits.flatten(0, 1),\n",
        "      target_batch.flatten(),\n",
        "  )\n",
        "  return loss"
      ],
      "metadata": {
        "id": "NUMWkVvfc0d3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
        "  \"\"\"\n",
        "  Calculates the loss for all batches in a data loader.\n",
        "  \"\"\"\n",
        "  total_loss = 0\n",
        "  if len(data_loader) == 0:\n",
        "    return float(\"nan\")\n",
        "  elif num_batches is None:\n",
        "    num_batches = len(data_loader)\n",
        "  else:\n",
        "    num_batches = min(num_batches, len(data_loader))\n",
        "\n",
        "  for index, (input_batch, target_batch) in enumerate(data_loader):\n",
        "    if index < num_batches:\n",
        "      loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "      total_loss += loss.item()\n",
        "    else:\n",
        "      break\n",
        "  return total_loss / num_batches # Return the average across all batches"
      ],
      "metadata": {
        "id": "VCuWJFD-dJPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test out the loss function\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "  train_loss = calc_loss_loader(train_loader, model, device)\n",
        "  val_loss = calc_loss_loader(val_loader, model, device)\n",
        "\n",
        "print(f\"Train loss: {train_loss}\")\n",
        "print(f\"Val loss: {val_loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tqg_VRXOdcXb",
        "outputId": "8fc777af-d53f-4867-c77e-aed2e35b2daa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 10.985362195420539\n",
            "Val loss: 10.984864807128906\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 15. Implement training functions"
      ],
      "metadata": {
        "id": "WrrESn4EhUlx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
        "  \"\"\"\n",
        "  Evaluates the model, giving estimate of model's training progress.\n",
        "  \"\"\"\n",
        "  # Put the model in evaluation mode\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "      train_loss = calc_loss_loader(\n",
        "          train_loader, model, device, num_batches=eval_iter\n",
        "      )\n",
        "      val_loss = calc_loss_loader(\n",
        "          val_loader, model, device, num_batches=eval_iter\n",
        "      )\n",
        "  # Put the model back in training mode\n",
        "  model.train()\n",
        "  return train_loss, val_loss"
      ],
      "metadata": {
        "id": "nOiliOoniadv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
        "  \"\"\"\n",
        "  Generates a sample text using the model.\n",
        "  \"\"\"\n",
        "  # Put the model in evaluation mode\n",
        "  model.eval()\n",
        "  context_size = model.positional_embedding.weight.shape[0]\n",
        "  encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
        "  with torch.no_grad():\n",
        "      token_ids = generate_text_simple(\n",
        "          model=model, idx=encoded,\n",
        "          max_new_tokens=50, context_size=context_size\n",
        "      )\n",
        "      decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
        "      print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
        "  # Put the model back in training mode\n",
        "  model.train()"
      ],
      "metadata": {
        "id": "Z3FGeN2-k8LV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_simple(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    optimizer,\n",
        "    device,\n",
        "    num_epochs,\n",
        "    eval_freq,\n",
        "    eval_iter,\n",
        "    start_context,\n",
        "    tokenizer,\n",
        "):\n",
        "  \"\"\"\n",
        "  Trains the model.\n",
        "  \"\"\"\n",
        "  # Initialize variables\n",
        "  train_losses, val_losses, track_tokens_seen = [], [], []\n",
        "  tokens_seen, global_step = 0, -1\n",
        "\n",
        "  # The main training loop\n",
        "  for epoch in range(num_epochs):\n",
        "    # Put model in training mode\n",
        "    model.train()\n",
        "\n",
        "    # Iterate through batches\n",
        "    for input_batch, target_batch in train_loader:\n",
        "      # Reset loss gradients from previous epoch\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # Calculate loss for batch\n",
        "      loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "\n",
        "      # Calculate loss gradients using backprop\n",
        "      loss.backward()\n",
        "\n",
        "      # Update model weights using loss gradients\n",
        "      optimizer.step()\n",
        "\n",
        "      # Update global variables\n",
        "      tokens_seen += input_batch.numel()\n",
        "      global_step += 1\n",
        "\n",
        "      # Evaluation step (optional)\n",
        "      if global_step % eval_freq == 0:\n",
        "        # Evaluate the model\n",
        "        train_loss, val_loss = evaluate_model(\n",
        "            model, train_loader, val_loader, device, eval_iter\n",
        "        )\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        track_tokens_seen.append(tokens_seen)\n",
        "        # print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
        "        #       f\"Train loss {train_loss:.3f}, \"\n",
        "        #       f\"Val loss {val_loss:.3f}\"\n",
        "        # )\n",
        "\n",
        "    generate_and_print_sample(\n",
        "        model, tokenizer, device, start_context\n",
        "    )\n",
        "  return train_losses, val_losses, track_tokens_seen\n"
      ],
      "metadata": {
        "id": "SHKm3TzZheOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 16. Train the model"
      ],
      "metadata": {
        "id": "ttNR5YMDl7AL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=0.0004,\n",
        "    weight_decay=0.1,\n",
        ")\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "start_time = time.time()\n",
        "train_losses, val_losses, tokens_seen = train_model_simple(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    optimizer,\n",
        "    device,\n",
        "    num_epochs=num_epochs,\n",
        "    eval_freq=5,\n",
        "    eval_iter=1,\n",
        "    start_context=\"Fiscal year 2023 included\",\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "end_time = time.time()\n",
        "print(f\"Training time: {end_time - start_time}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7ny_Rn-l891",
        "outputId": "9b341755-9515-47f6-e5de-0c72330d1830"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fiscal year 2023 included the Company. We. We. We. We. We the Company. We. We the Company. We. We. We. We. We. We. We. We. We. We. We. We. We. We. We\n",
            "Fiscal year 2023 included. We have a variety of our products and other cloud services. Our products and other cloud services. Our Company’s products and other cloud services. We also of our products and other services. Our products and other cloud services. Our products and\n",
            "Fiscal year 2023 included a wide impact of the Company’s, and other cloud services.                                   \n",
            "Fiscal year 2023 included a year 2022, and the Company’s products and services. The Company’s products and services. The Company’s products and services. The Company’s products and services and the Company’s products and services.\n",
            "Fiscal year 2023 included in China net sales, 2022.0.3 billion during 2023 compared to 2022. The following table year ago reflecting the year 2023, partially offset by lower net sales of the weakness in foreign currencies relative to the U.0 billion in millions\n",
            "Fiscal year 2023 included a global of total pay for certain equipment, and 2022, and 2022.S.S.S.S.S., and 2022 due to the U.S DISC’s financial condition and 2022 due to help reduce channel partners to help\n",
            "Fiscal year 2023 included a significantly for fiscal year 2022. Fiscal Year 2023 was $1.S.S.S.S.S.S.S. dollar and 2022  •Increasing% of fiscal year 2023 compared to 2022, the U.S\n",
            "Fiscal year 2023 included in 2022. The Company’s Discussion and information across the Company’s Discussion and 2021 (“MD&A”) is intended to help the reader understand the Company”) for the Company also obtains individual components for\n",
            "Fiscal year 2023 included in $13.6 billion during 2023, the entire year-term competitiveness, and administrative to the U.S.S.S.S., and to the U.S.S.S.S.S.S.S.\n",
            "Fiscal year 2023 included a significantly for fiscal year 2022. Fiscal year 2023 metrics are expected to be published in fiscal year 2023 reflecting the write-off of the prepayment provided at signing. Fiscal Year 2023 Summary   | Year Ended   | January\n",
            "Training time: 61.629645109176636\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
        "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
        "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
        "    ax1.plot(\n",
        "        epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\"\n",
        "    )\n",
        "    ax1.set_xlabel(\"Epochs\")\n",
        "    ax1.set_ylabel(\"Loss\")\n",
        "    ax1.legend(loc=\"upper right\")\n",
        "    ax2 = ax1.twiny()\n",
        "    ax2.plot(tokens_seen, train_losses, alpha=0)\n",
        "    ax2.set_xlabel(\"Tokens seen\")\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
        "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "wPYbiSbumU5s",
        "outputId": "2b185b6c-f5d6-430c-a8cd-73e821a7a367"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x300 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total number of parameters: {total_params:,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6SVncgcnoUz",
        "outputId": "e90a7261-92ae-48d5-c836-ffb1dcb0bd0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of parameters: 162,419,712\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_params_gpt2 = (\n",
        "    total_params - sum(p.numel()\n",
        "    for p in model.out_head.parameters())\n",
        ")\n",
        "print(f\"Number of trainable parameters \"\n",
        "      f\"considering weight tying: {total_params_gpt2:,}\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YswtvXvTnzwz",
        "outputId": "f3055838-1457-4315-bf3e-1b92a73ffb31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of trainable parameters considering weight tying: 123,822,336\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 17. Add temperature (randomness) parameter"
      ],
      "metadata": {
        "id": "UqrQ_RXaoJHj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Put model in eval mode on CPU since inference does not require compute\n",
        "model.to(\"cpu\")\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "sITxFsFSo9yp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e71c907-7b5e-4885-d89d-646af90d3e48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPTModel(\n",
              "  (token_embedding): Embedding(50257, 768)\n",
              "  (positional_embedding): Embedding(256, 768)\n",
              "  (drop_embedding): Dropout(p=0.1, inplace=False)\n",
              "  (transformer_blocks): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (_out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (_out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (2): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (_out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (3): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (_out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (4): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (_out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (5): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (_out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (6): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (_out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (7): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (_out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (8): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (_out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (9): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (_out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (10): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (_out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (11): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (_out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (final_norm): LayerNorm()\n",
              "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "token_ids = generate_text_simple(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(\"Fiscal year 2023 included\", tokenizer),\n",
        "    max_new_tokens=25,\n",
        "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
        ")\n",
        "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hw5h1ulqpCE1",
        "outputId": "6bddc634-d654-44da-8b5e-d22bca7eb760"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output text:\n",
            " Fiscal year 2023 included a significantly for fiscal year 2022. Fiscal year 2023 metrics are expected to be published in fiscal year 2023 reflecting the write\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 17. Save the model"
      ],
      "metadata": {
        "id": "0_ql6Y3GpGCS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save({\n",
        "  \"model_state_dict\": model.state_dict(), # the model's layer -> parameter state\n",
        "  \"optimizer_state_dict\": optimizer.state_dict(), # the AdamW optimizer's state\n",
        "  },\n",
        "  \"model_and_optimizer.pth\"\n",
        ")"
      ],
      "metadata": {
        "id": "uMpQmjQXaWvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 18. Load the saved model"
      ],
      "metadata": {
        "id": "Uh473ACwabgH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = torch.load(\"model_and_optimizer.pth\")\n",
        "\n",
        "# Load the model\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "\n",
        "# Load the optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)\n",
        "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "\n",
        "# Switch model to evaluation mode, which disables dropout\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "UeLrasHDago1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2eef6e0-8e2a-415b-8b00-cddbd292530c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPTModel(\n",
              "  (token_embedding): Embedding(50257, 768)\n",
              "  (positional_embedding): Embedding(256, 768)\n",
              "  (drop_embedding): Dropout(p=0.1, inplace=False)\n",
              "  (transformer_blocks): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (_out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (_out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (2): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (_out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (3): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (_out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (4): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (_out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (5): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (_out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (6): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (_out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (7): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (_out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (8): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (_out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (9): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (_out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (10): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (_out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (11): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (_out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (final_norm): LayerNorm()\n",
              "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PMUB6AORaleO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}